
# ğŸš€ AWS Redshift Data Warehouse Pipeline â€“ by Samratha Reddy

This project is a **modern data warehouse pipeline** designed and customized by **Samratha Reddy**, focused on ingesting, transforming, and loading data into **Amazon Redshift** using **Python and AWS services**. The solution mimics real-world enterprise architecture for batch ETL processing.

**GitHub Repository:** https://github.com/samrathareddy/AWS-Redshift-Data-Warehouse-Pipeline.git 
---

## ğŸ“Œ Project Highlights

- Built a scalable ETL pipeline from scratch using Python.
- Created and configured AWS Redshift clusters programmatically.
- Uploaded staging and production data from S3 to Redshift.
- Used SQL for table creation, data modeling, and analytics.
- Emulated real business use cases such as user activity logs and song metadata processing.

---

## ğŸ› ï¸ Tech Stack

- **Language:** Python 3
- **Cloud:** AWS Redshift, S3, IAM
- **Libraries:** psycopg2, configparser
- **Data:** Log and song JSON files
- **Tools:** Git, Jupyter Notebook, SQL

---

## ğŸ“ Folder Structure

```
AWS-Redshift-Pipeline/
â”œâ”€â”€ create_tables.py           # Drops & creates tables in Redshift
â”œâ”€â”€ etl.py                     # Loads data into staging & analytics tables
â”œâ”€â”€ dwh.cfg                    # AWS Redshift and S3 configurations
â”œâ”€â”€ sql_queries.py             # SQL statements for ETL process
â”œâ”€â”€ README.md                  # Project overview and documentation
â””â”€â”€ data/                      # Contains sample input files (optional)
```

---

## ğŸš€ Setup Instructions

1. **Clone the repo**
   ```bash
   git clone https://github.com/samrathareddy/AWS-Redshift-Pipeline.git
   cd AWS-Redshift-Pipeline
   ```

2. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

3. **Configure AWS and Redshift**
   - Edit the `dwh.cfg` file with your Redshift cluster and IAM role credentials.

4. **Run the pipeline**
   ```bash
   python create_tables.py
   python etl.py
   ```

---

## ğŸ‘¨â€ğŸ’» About Me â€“ Samratha Reddy

A passionate **Data Engineer and Graduate Student at Cleveland State University**, I build robust data pipelines, streaming architectures, and cloud-based ETL systems using Python, AWS, and SQL. This project reflects my practical knowledge of Redshift-based architectures.

ğŸ“¬ Connect with me:  
ğŸ”— [LinkedIn](https://www.linkedin.com/in/samrathareddy)

---

## ğŸ“Š Use Cases Simulated

- Analytics on user activity logs
- ETL from S3 â†’ Redshift staging â†’ analytics schema
- Reusable and modular SQL logic
- IAM role-based secure data access

---

## ğŸ“Œ Future Improvements

- Integrate with Apache Airflow for orchestration
- Add test coverage and CI/CD pipeline
- Convert staging to Parquet-based optimized loading

---

â­ **If you find this useful, give it a star and fork it!**
